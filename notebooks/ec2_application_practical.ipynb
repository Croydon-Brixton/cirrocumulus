{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Computing Fundamentals Practical\n",
    "## Running an application on EC2\n",
    "### By: Eric Meissner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "* AWS Account created\n",
    "* Boto3\n",
    "* AWS CLI - https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html\n",
    "\n",
    "``` pip install boto3```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3\n",
    "* Make a bucket\n",
    "* Upload some data and a training.py file to that bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2\n",
    "* Spin up an instance from SDK, explain AMIs (Virtual Machine images), networking / firewall, availability zones + regions (here or later?, the logic behind them)\n",
    "* SSH to EC2 instance, SCP over the training.py file\n",
    "* Spin up Jupyter notebook on instance + access from local machine\n",
    "* Spin up the training application and call it from your local machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Login to the AWS Console \n",
    "\n",
    "### 1. Create new EC2 instance (from the console)\n",
    "1. Click on (in the top left) Services -> EC2.\n",
    "2. Click on \"Instances\" in the left sidebar, then \"Launch instances\" on the right side.\n",
    "3. This brings us to a page where you select what AMI to use as the base for the machine. An AMI is essentially packaged base operating system, and may have some additional packages installed depending on the AMI you choose. \n",
    "  1. Let's choose the ```Ubuntu Server 18.04 LTS (HVM), SSD Volume Type - ami-0bcc094591f354be2``` because it's free tier eligible. \n",
    "  2. The Deep Learning AMIs come pre-installed with drivers such as NVidia GPU drivers and common DL frameworks such as Tensorflow, Pytorch, and MXNet. The \"Deep Learning Base\" AMI sets come with only the additional drivers installed, but not the frameworks. These are useful starting points for most applications, as configuring drivers can be somewhat of a hassle.\n",
    "4. The next screen brings us to the choice of hardware we want to run. Typically, you'll choose this based on what the application's needs are (such as whether you need a GPU, high memory, etc.) In this example, we'll choose a very small instances, the ```t2.micro``` because it's eligible for free tier status. \n",
    "  1. Note: this is a helpful website for looking at the stats of EC2 instances https://www.ec2instances.info/.\n",
    "5. Click on \"Configure Instance Details\" next. \n",
    "  1. We don't need to change anything here, but notice that the instance is created inside your account's default \"network\" or VPC. All accounts start with a default VPC, and all instances live within some VPC and this controls what network traffic goes into and out of the instance. An important aspect of subnets is defining which Availability Zone an instance is launched into. \n",
    "6. Click \"Add Storage\". Here you can increase or decrease the amount of block storage (EBS) your instance will have when spun up. You can change this later, so we can leave it at default for now.\n",
    "7. Click through \"Add Tags\" (they are just key-value pair meta-data additions for the instance to help you manage them), and click on \"Configure Security Groups\".\n",
    "  1. Remember that Security Groups essentially define firewall rules for instances inside of that security group. If you don't place an EC2 instance into a security group, anyone can access this machine if they have the correct PEM key (which we get in the next step.) For most production machines, you'll want access to be tightly restricted, typically to only other machines within the subnet and if they're serving web traffic then only full access rules on the HTTP/HTTPS ports.\n",
    "  2. For today's uses though, we'll simply add a security group that allows SSH access from any address. This should already be populated when you've selected the \"Create New Security Group\" option. SSH's port is 22, and we allow access from 0.0.0.0/0, i.e. any address. \n",
    "8. Click through \"Review and Launch\" and look over the information on the page to make sure it's alright. It will show a warning up top regarding our security groups, this is fine. \n",
    "9. When you click Launch, it will prompt you to select or create a keypair. If you already have one, feel free to use that, otherwise create a keypair with a useful name (often people have a personal keypair for their instances (\"bob_keypair\"), and then separate ones for collective instances like \"ml_training_instances_keypair\".) You'll then need to download and **not lose** this keypair, as you can't get it back and any instance launched using the keypair will require you to have it to access via SSH.\n",
    "10. Once you've downloaded your new keypair though, launch the instance! You can go back to the instances page to see the instance launching (they take a few minutes to initialize.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an IAM User to login from Boto3 / console\n",
    "IAM controls access to AWS resources. There are IAM users and IAM roles, and we'll need to create a user in order to access our AWS account from the command line. IAM Roles are *assumed* by AWS resources when they run that allow more natural access to other resources. I.e. an EC2 host can assume an IAM role to access S3 buckets without needing user keys explicitly setup.\n",
    "\n",
    "1. From the console, Services -> IAM.\n",
    "2. Add User -> Fill in a username -> Tick the \"Programmatic box\", you won't need the \"AWS Console\" one. -> Next: Permissions\n",
    "3. Click create group (ex. name: 'cloud_computing_module'), so that you can create new users in the future with the same policies already setup. Initially, we will want to create an S3 bucket and add to it from the command line, so add the ```AmazonS3FullAccess``` policy.\n",
    "4. Click through tags and create your new user!\n",
    "5. When you get to the \"Success!\" screen, click \"download .csv\" to download the credential pair. This consists of an Access Key and an Secret Access Key.\n",
    "6. From a terminal, run ```aws configure``` and input the access key and secret access keys when prompted.\n",
    "7. You're good to go now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup S3 Bucket with Boto3\n",
    "Boto3 is the Python library for accessing and managing AWS services. Below I give a basic use case for creating an S3 bucket. The documentation is really good for it, so just google whatever you're trying to do with it.\n",
    "\n",
    "Note: \n",
    "- The bucket name must be globally unique. If another user created a bucket with the name you specified, you will receiva a \"BucketAlreadyExists\" error.\n",
    "- For any region other than `us-east-1` you need to add the `location-constraint` when adding the bucket.\n",
    "\n",
    "Here's [more info](https://realpython.com/python-boto3-aws-s3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T19:57:51.220974Z",
     "start_time": "2020-12-22T19:57:51.217807Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T19:59:32.447778Z",
     "start_time": "2020-12-22T19:59:32.338035Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3 \n",
    "session = boto3.session.Session()\n",
    "s3_connection = session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T19:59:52.353160Z",
     "start_time": "2020-12-22T19:59:52.348544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud-computing-module-62699643697040fcad7801ab7dac3680\n"
     ]
    }
   ],
   "source": [
    "# Make sure to create a bucket with an appropriate name\n",
    "bucket_name = \"cloud-computing-module-\" + uuid.uuid4().hex\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T19:59:57.850065Z",
     "start_time": "2020-12-22T19:59:56.654567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Bucket(name='cloud-computing-module-62699643697040fcad7801ab7dac3680')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_connection.create_bucket(Bucket=bucket_name,\n",
    "                          CreateBucketConfiguration={\n",
    "                        'LocationConstraint': session.region_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T20:16:09.110118Z",
     "start_time": "2020-12-22T20:16:09.104562Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket = s3_connection.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T20:16:09.473971Z",
     "start_time": "2020-12-22T20:16:09.470191Z"
    }
   },
   "outputs": [],
   "source": [
    "# the location of the object within the bucket\n",
    "key = \"data.txt\"\n",
    "# the content of the object\n",
    "result = \"Hello World\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T20:16:10.915079Z",
     "start_time": "2020-12-22T20:16:10.002379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '10871CC88BC5F027',\n",
       "  'HostId': 'BbjUM4ITL/r2LGgeG+/5ZYv37mwimXVMgWjsD8U5KB5uDEnY70E74cfOJcoAL0/3Hp6BTvcMvsI=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'BbjUM4ITL/r2LGgeG+/5ZYv37mwimXVMgWjsD8U5KB5uDEnY70E74cfOJcoAL0/3Hp6BTvcMvsI=',\n",
       "   'x-amz-request-id': '10871CC88BC5F027',\n",
       "   'date': 'Tue, 22 Dec 2020 20:16:10 GMT',\n",
       "   'etag': '\"b10a8db164e0754105b7a99be72e3fe5\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"b10a8db164e0754105b7a99be72e3fe5\"'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the content to the specific location in the given bucket\n",
    "bucket.Object(key).put(Body=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSH to EC2 instance and run training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To SSH to your instance run something like the following using the keypair you created when launching your instance:\n",
    "\n",
    "```bash\n",
    "ssh -i <path/to/keypair.pem> ubuntu@<public_ip_of_your_ec2_instance>\n",
    "```\n",
    "\n",
    "You can find the public IP or dnsname of your instance by clicking on it from the console and finding the information.\n",
    "\n",
    "If you get an error about permissions or access priviledges when you first try to ssh, try running ```sudo chmod 700 <path/to/pem>``` on your .pem file to get it's permissions setup correctly..\n",
    "\n",
    "Also note that if you used something other than a Ubuntu machine, you may need to have a different prefix ```ec2-user@<IP>``` for Amazon Linux for example.\n",
    "\n",
    "You can use the ```scp``` command to move files between your machine and the EC2 instance now."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
